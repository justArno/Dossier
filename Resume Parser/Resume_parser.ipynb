{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# resumeObj = open('resume.pdf', 'rb')\n",
    "# pdfReader = PyPDF2.PdfFileReader(resumeObj)\n",
    "# # printing number of pages in pdf file\n",
    "# print(pdfReader.numPages)\n",
    "# # creating a page object\n",
    "# pageObj = pdfReader.getPage(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Arjun C \n",
      "Menon\n",
      " \n",
      "Contact\n",
      " \n",
      "Address:\n",
      " \n",
      "303, Ravoos\n",
      " \n",
      "Pansy, Munekolala, \n",
      "Bangalore 560037\n",
      " \n",
      " \n",
      "Phone:\n",
      " \n",
      "+\n",
      "91 9945095002\n",
      " \n",
      " \n",
      " \n",
      "Email:\n",
      " \n",
      "arjunmenonc@gmail.com\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Energetic and passionate college student\n",
      " \n",
      "currently\n",
      " \n",
      "working towards \n",
      "a        \n",
      "B \n",
      "-\n",
      "Tech degree at PESU ECC.\n",
      " \n",
      "Hard working, inquisitive individual with                                                                         \n",
      "a passion to learn\n",
      " \n",
      "and experiment.\n",
      " \n",
      "Skill Highlights\n",
      " \n",
      "\n",
      " \n",
      "Project management\n",
      " \n",
      "\n",
      " \n",
      "Strong decision maker\n",
      " \n",
      "\n",
      " \n",
      "Complex problem solver\n",
      " \n",
      "\n",
      " \n",
      "Innovative\n",
      " \n",
      "\n",
      " \n",
      "Creative Thin\n",
      "ker\n",
      " \n",
      "Volunteer \n",
      "Experience\n",
      " \n",
      "or Leadership\n",
      " \n",
      "\n",
      " \n",
      "Worked as an intern at \n",
      "\n",
      "Utkarshin\n",
      "\n",
      "Assistant specializing in Machine Learning and NMT. \n",
      " \n",
      "(March \n",
      "\n",
      " \n",
      "May 2020)\n",
      " \n",
      "\n",
      " \n",
      "3\n",
      "rd\n",
      " \n",
      "Level Qualifier in E\n",
      "-\n",
      "Yantra Robotics Competition 2019\n",
      " \n",
      "\n",
      " \n",
      "20\n",
      " \n",
      "\n",
      " \n",
      "All India Semi \n",
      "\n",
      " \n",
      "Finalist in Heritage India Quiz 2015 \n",
      "-\n",
      " \n",
      "16\n",
      " \n",
      "\n",
      " \n",
      "Competed in \n",
      "hackathons and competitions\n",
      " \n",
      "o\n",
      " \n",
      "Decypher organized by PES\n",
      " \n",
      "o\n",
      " \n",
      "Kalpana organized by PES\n",
      " \n",
      "\n",
      " \n",
      "Worked \n",
      "on events a\n",
      "nd projects under the \n",
      "organization\n",
      " \n",
      "of \n",
      "Ad\n",
      "vanced \n",
      "Computing Machiner\n",
      "y (\n",
      "AC\n",
      "M\n",
      ")\n",
      " \n",
      "Education\n",
      " \n",
      "B\n",
      "Tech in CSE: 2022, PES University E\n",
      "lectronic City \n",
      " \n",
      "Class XII: 2018, Sri Sri Ravishankar Vidya Mandir\n",
      " \n",
      "Class X: 2016, Sri Sri Ravishankar Vidya Mandir\n",
      " \n",
      " \n",
      "Programming Languages: \n",
      "C, C++, Python, R, \n",
      "JavaScript, HTML5, PHP \n",
      "OOP, CSS, SQL, MySQL\n",
      ".\n",
      " \n",
      " \n",
      "Projects\n",
      " \n",
      "\n",
      " \n",
      "H\n",
      "ome Credit Default Risk (\n",
      "Kaggle 2018\n",
      ")\n",
      " \n",
      "(\n",
      "Python\n",
      ")\n",
      " \n",
      "\n",
      " \n",
      "High Accuracy ALPR (\n",
      "2019\n",
      ")\n",
      " \n",
      "(\n",
      "Python\n",
      ")\n",
      " \n",
      "\n",
      " \n",
      "Smart Camera with Ob\n",
      "ject/ Fa\n",
      "ce Detection\n",
      " \n",
      "(\n",
      "2020\n",
      ")\n",
      " \n",
      "(\n",
      "P\n",
      "ython\n",
      ")\n",
      " \n",
      "\n",
      " \n",
      "Fac\n",
      "e Mas\n",
      "k Detection and Soc\n",
      "ial \n",
      "Distanci\n",
      "ng Meters for the \n",
      "blind \n",
      "(\n",
      "2020\n",
      ")\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(pageObj.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINSTALLATIONS\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "INSTALLATIONS\n",
    "'''\n",
    "#pip install pdfminer.six\n",
    "#pip install docx2txt\n",
    "#pip install nltk\n",
    "#pip install numpy\n",
    "#pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Arjun C \n",
      "\n",
      "Menon \n",
      "\n",
      " \n",
      "\n",
      "Energetic and passionate college student currently working towards a        \n",
      "B -Tech degree at PESU ECC. Hard working, inquisitive individual with                                                                         \n",
      "a passion to learn and experiment. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Volunteer Experience or Leadership \n",
      "\n",
      "Contact \n",
      "\n",
      "Address: \n",
      "303, Ravoos Pansy, Munekolala, \n",
      "Bangalore 560037 \n",
      " \n",
      "Phone: \n",
      "+91 9945095002  \n",
      " \n",
      "Email: \n",
      "arjunmenonc@gmail.com \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Skill Highlights \n",
      "\n",
      "•  Project management \n",
      "•  Strong decision maker \n",
      "•  Complex problem solver \n",
      "\n",
      "• \n",
      "Innovative \n",
      "•  Creative Thinker \n",
      "\n",
      "•  Worked as an intern at “Utkarshini Edutech” as a Teaching \n",
      "\n",
      "Assistant specializing in Machine Learning and NMT.  (March \n",
      "– May 2020) \n",
      "\n",
      "•  3rd Level Qualifier in E-Yantra Robotics Competition 2019 – \n",
      "\n",
      "20 \n",
      "\n",
      "•  All India Semi – Finalist in Heritage India Quiz 2015 - 16 \n",
      "•  Competed in hackathons and competitions \n",
      "o  Decypher organized by PES \n",
      "o  Kalpana organized by PES \n",
      "\n",
      "•  Worked on events and projects under the organization of \n",
      "\n",
      "Advanced Computing Machinery (ACM) \n",
      "\n",
      "Education \n",
      "\n",
      "BTech in CSE: 2022, PES University Electronic City  \n",
      "Class XII: 2018, Sri Sri Ravishankar Vidya Mandir \n",
      "Class X: 2016, Sri Sri Ravishankar Vidya Mandir \n",
      " \n",
      "Programming Languages: C, C++, Python, R, JavaScript, HTML5, PHP \n",
      "OOP, CSS, SQL, MySQL. \n",
      " \n",
      "Projects \n",
      "\n",
      "•  Home Credit Default Risk (Kaggle 2018) (Python) \n",
      "•  High Accuracy ALPR (2019) (Python) \n",
      "•  Smart Camera with Object/ Face Detection (2020) (Python) \n",
      "•  Face Mask Detection and Social Distancing Meters for the \n",
      "\n",
      "blind (2020) \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Extracting text from PDF files'''\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('resume.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arjun\n"
     ]
    }
   ],
   "source": [
    "'''Extracting names from resumes'''\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('resume.pdf')\n",
    "    names = extract_names(text)\n",
    "\n",
    "    if names:\n",
    "        print(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+91 9945095002\n"
     ]
    }
   ],
   "source": [
    "'''Extracting phone numbers from resumes'''\n",
    "\n",
    "\n",
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "\n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  # noqa: S607,S603\n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    return (stdout.strip(), stderr.strip())\n",
    "\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    phone_number = extract_phone_number(text)\n",
    "\n",
    "    print(phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arjunmenonc@gmail.com\n"
     ]
    }
   ],
   "source": [
    "'''Extracting email addresses from resumes'''\n",
    "\n",
    "import re\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('resume.pdf')\n",
    "    emails = extract_emails(text)\n",
    "\n",
    "    if emails:\n",
    "        print(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid authentication credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-717a1da4e42d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mskills\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskills\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa: T001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-717a1da4e42d>\u001b[0m in \u001b[0;36mextract_skills\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# we search for each token in our skills database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mskill_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mfound_skills\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-717a1da4e42d>\u001b[0m in \u001b[0;36mskill_exists\u001b[1;34m(skill)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mskill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Invalid authentication credentials"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# '''Extracting skills from the resumes with skillAPI'''\n",
    "\n",
    "# import docx2txt\n",
    "# import nltk\n",
    "# import requests\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "# def skill_exists(skill):\n",
    "#     url = f'https://api.promptapi.com/skills?q={skill}&count=1'\n",
    "#     headers = {'apikey': 'YOUR API KEY'}\n",
    "#     response = requests.request('GET', url, headers=headers)\n",
    "#     result = response.json()\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return len(result) > 0 and result[0].lower() == skill.lower()\n",
    "#     raise Exception(result.get('message'))\n",
    "\n",
    "\n",
    "# def extract_skills(input_text):\n",
    "#     stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#     word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "#     # remove the stop words\n",
    "#     filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "#     # remove the punctuation\n",
    "#     filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "#     # generate bigrams and trigrams (such as artificial intelligence)\n",
    "#     bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "#     # we create a set to keep the results in.\n",
    "#     found_skills = set()\n",
    "\n",
    "#     # we search for each token in our skills database\n",
    "#     for token in filtered_tokens:\n",
    "#         if skill_exists(token.lower()):\n",
    "#             found_skills.add(token)\n",
    "\n",
    "#     # we search for each bigram and trigram in our skills database\n",
    "#     for ngram in bigrams_trigrams:\n",
    "#         if skill_exists(ngram.lower()):\n",
    "#             found_skills.add(ngram)\n",
    "\n",
    "#     return found_skills\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     skills = extract_skills(text)\n",
    "\n",
    "#     print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Machine Learning', 'Python'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# we may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PES University Electronic City'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Extracting education and schools from resumes'''\n",
    "\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word) >= 0:\n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    education_information = extract_education(text)\n",
    "\n",
    "    print(education_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
